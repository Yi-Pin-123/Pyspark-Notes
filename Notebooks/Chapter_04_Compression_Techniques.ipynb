{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCompression:\n",
    "    \"\"\"\n",
    "    Class to test compression and decompression benchmarks in Apache Spark.\n",
    "    \"\"\"\n",
    "    file_name: str\n",
    "    df: \"pyspark.sql.DataFrame\"\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_execution_time(func):\n",
    "        \"\"\"Decorator to measure function execution time.\"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"{func.__name__}: {execution_time:.4f} seconds\")\n",
    "            return result, execution_time\n",
    "        return wrapper\n",
    "\n",
    "    @measure_execution_time\n",
    "    def test_compression(self, format: str, compression: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Test compression by writing a DataFrame to disk.\n",
    "        \"\"\"\n",
    "        output_path = f\"output/{format}_{compression}\"\n",
    "\n",
    "        # Ensure output directory is empty before writing\n",
    "        if os.path.exists(output_path):\n",
    "            for file in os.listdir(output_path):\n",
    "                os.remove(os.path.join(output_path, file))\n",
    "\n",
    "        # Write DataFrame\n",
    "        self.df.write.mode(\"overwrite\").format(format).option(\"compression\", compression).save(output_path)\n",
    "\n",
    "        # Ensure directory exists before calculating size\n",
    "        file_size_mb = 0.0\n",
    "        if os.path.exists(output_path):\n",
    "            file_size = sum(\n",
    "                os.path.getsize(os.path.join(output_path, f))\n",
    "                for f in os.listdir(output_path) if os.path.isfile(os.path.join(output_path, f))\n",
    "            )\n",
    "            file_size_mb = round(file_size / (1024 * 1024), 2)  # Convert to MB\n",
    "\n",
    "        return file_size_mb\n",
    "\n",
    "    @measure_execution_time\n",
    "    def test_decompression(self, format: str, compression: str) -> None:\n",
    "        \"\"\"\n",
    "        Test decompression by reading a DataFrame from disk.\n",
    "        \"\"\"\n",
    "        input_path = f\"output/{format}_{compression}\"\n",
    "        _ = spark.read.format(format).load(input_path).count()\n",
    "\n",
    "    def test_compression_benchmarks(self):\n",
    "        \"\"\"\n",
    "        Run compression and decompression benchmarks for Parquet using Zstd, Snappy, and LZ4.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        codecs = [\"zstd\", \"snappy\", \"lz4\"]\n",
    "\n",
    "        for codec in codecs:\n",
    "            # Test compression\n",
    "            size, compression_time = self.test_compression(\"parquet\", codec)\n",
    "            \n",
    "            # Test decompression\n",
    "            _, decompression_time = self.test_decompression(\"parquet\", codec)\n",
    "            \n",
    "            results[f\"parquet_{codec}\"] = {\n",
    "                \"size_mb\": size,\n",
    "                \"compression_time_seconds\": compression_time,\n",
    "                \"decompression_time_seconds\": decompression_time\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_original_file_size(self) -> float:\n",
    "        \"\"\"\n",
    "        Get the size of the original CSV file.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.file_name):\n",
    "            file_size = os.path.getsize(self.file_name)\n",
    "            return round(file_size / (1024 * 1024), 2)  # Convert to MB\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_compression: 3.3148 seconds\n",
      "test_decompression: 0.2785 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_compression: 3.6613 seconds\n",
      "test_decompression: 0.2938 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_compression: 2.9821 seconds\n",
      "test_decompression: 0.2153 seconds\n",
      "\n",
      "Original CSV file size: 70.91 MB\n",
      "\n",
      "üî• Compression and Decompression Benchmark Results üî• \n",
      "\n",
      "parquet_zstd: \n",
      " \n",
      "  Compression Time: 3.31s\n",
      "  Decompression Time: 0.28s\n",
      "  Size: 10.73 MB\n",
      "\n",
      "parquet_snappy: \n",
      " \n",
      "  Compression Time: 3.66s\n",
      "  Decompression Time: 0.29s\n",
      "  Size: 22.54 MB\n",
      "\n",
      "parquet_lz4: \n",
      " \n",
      "  Compression Time: 2.98s\n",
      "  Decompression Time: 0.22s\n",
      "  Size: 22.96 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"CompressionBenchmark\").getOrCreate()\n",
    "\n",
    "    # Path to CSV file\n",
    "    FILE_PATH = \"/workspaces/high-performance-pyspark-advanced-strategies-for-optimal-data-processing-3919191/data/test_data.csv\"\n",
    "\n",
    "    # Read CSV into DataFrame with correct data types\n",
    "    df = spark.read.option(\"header\", \"true\").csv(FILE_PATH)\n",
    "    \n",
    "    # Ensure DataFrame is not empty\n",
    "    if df.count() == 0:\n",
    "        print(\"‚ùå ERROR: DataFrame is empty. Please check the input CSV file.\")\n",
    "        spark.stop()\n",
    "        exit()\n",
    "\n",
    "    # Get original file size before compression\n",
    "    test_compression = TestCompression(file_name=FILE_PATH, df=df)\n",
    "    original_size_mb = test_compression.get_original_file_size()\n",
    "\n",
    "    # Run compression and decompression benchmarks\n",
    "    results = test_compression.test_compression_benchmarks()\n",
    "\n",
    "    # Print original file size and benchmark results\n",
    "    print(f\"\\nOriginal CSV file size: {original_size_mb} MB\\n\")\n",
    "    print(\"üî• Compression and Decompression Benchmark Results üî• \\n\")\n",
    "    for algorithm, metrics in results.items():\n",
    "        print(f\"{algorithm}: \\n \")\n",
    "        print(f\"  Compression Time: {metrics['compression_time_seconds']:.2f}s\")\n",
    "        print(f\"  Decompression Time: {metrics['decompression_time_seconds']:.2f}s\")\n",
    "        print(f\"  Size: {metrics['size_mb']} MB\\n\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
