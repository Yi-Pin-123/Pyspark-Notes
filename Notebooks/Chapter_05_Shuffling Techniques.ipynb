{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 07:32:22 WARN Utils: Your hostname, codespaces-e1abf6 resolves to a loopback address: 127.0.0.1; using 10.0.0.54 instead (on interface eth0)\n",
      "25/05/18 07:32:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/18 07:32:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Shuffle Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with sample data\n",
    "data = [\n",
    "    (1, \"Alice\", 100),\n",
    "    (2, \"Bob\", 200),\n",
    "    (3, \"Alice\", 150),\n",
    "    (4, \"Bob\", 250),\n",
    "    (5, \"Charlie\", 300)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"amount\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"name\" and sum the \"amount\" to force a shuffle\n",
    "grouped_df = df.groupBy(\"name\").sum(\"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['name], ['name, sum(amount#2L) AS sum(amount)#10L]\n",
      "+- LogicalRDD [id#0L, name#1, amount#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, sum(amount): bigint\n",
      "Aggregate [name#1], [name#1, sum(amount#2L) AS sum(amount)#10L]\n",
      "+- LogicalRDD [id#0L, name#1, amount#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [name#1], [name#1, sum(amount#2L) AS sum(amount)#10L]\n",
      "+- Project [name#1, amount#2L]\n",
      "   +- LogicalRDD [id#0L, name#1, amount#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[name#1], functions=[sum(amount#2L)], output=[name#1, sum(amount)#10L])\n",
      "   +- Exchange hashpartitioning(name#1, 200), ENSURE_REQUIREMENTS, [plan_id=15]\n",
      "      +- HashAggregate(keys=[name#1], functions=[partial_sum(amount#2L)], output=[name#1, sum#14L])\n",
      "         +- Project [name#1, amount#2L]\n",
      "            +- Scan ExistingRDD[id#0L,name#1,amount#2L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.explain(mode=\"extended\")  # Use \"extended\" to get detailed plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query plans are read from bottom to top,indicating the flow of data through different operations.Indentation represents parallel operations,meaning those steps can occur simultaneously.\n",
    "\n",
    "In this line, Scan ExistingRDD,data is being loaded into Spark from an in-memory dataset.This is the starting point of the execution.In this line, only name and amount columns are selected,reducing unnecessary data movement.\n",
    "\n",
    "HashAggregate.This is the partial aggregation done before shuffle.Spark performs partial aggregation per partition.Instead of sending all records for a name,it partially sums value within each partition,reducing shuffle size.\n",
    "\n",
    "Exchange hashpartitioning.This is the shuffle stage.Spark redistributes data across 200 partitionsusing hash partitioning on name.Now all records with the same name are in the partition.\n",
    "\n",
    "final aggregation at HashAggregate.Spark performs the final aggregationon amount after the shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 07:32:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name|sum(amount)|\n",
      "+-------+-----------+\n",
      "|    Bob|        450|\n",
      "|  Alice|        250|\n",
      "|Charlie|        300|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trigger execution (e.g., show results)\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
