{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/14 01:22:44 WARN Utils: Your hostname, codespaces-976928 resolves to a loopback address: 127.0.0.1; using 10.0.2.76 instead (on interface eth0)\n",
      "25/02/14 01:22:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/14 01:22:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up PySpark Session\n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataSkewHandling\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a Sample DataFrame with Skew\n",
    "from pyspark.sql import functions as F\n",
    "# Create a DataFrame with skewed data\n",
    "data = [(1, \"A\")] * 1000 + [(2, \"B\")] * 100 + [(3, \"C\")] * 10\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\"])\n",
    "# Show the DataFrame\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows per partition:\n",
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0|  555|\n",
      "|                   1|  555|\n",
      "+--------------------+-----+\n",
      "\n",
      "\n",
      "Data in partitions (first 2 rows per partition):\n",
      "Partition 0: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 1: [Row(id=1, category='A'), Row(id=1, category='A')]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Diagnose Data Skew\n",
    "# Check the number of rows per partition\n",
    "print(\"\\nNumber of rows per partition:\")\n",
    "df.groupBy(F.spark_partition_id()).count().show()\n",
    "# Inspect data distribution in partitions\n",
    "print(\"\\nData in partitions (first 2 rows per partition):\")\n",
    "partitions = df.rdd.glom().collect()\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Partition {i}: {partition[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+-----+\n",
      "|partition_id| id|count|\n",
      "+------------+---+-----+\n",
      "|           0|  1|  555|\n",
      "|           1|  1|  445|\n",
      "|           1|  2|  100|\n",
      "|           1|  3|   10|\n",
      "+------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of 'id' across partitions\n",
    "df.withColumn(\"partition_id\", F.spark_partition_id()) \\\n",
    "  .groupBy(\"partition_id\", \"id\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"partition_id\", \"id\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use df.repartition function with number of partitionsand the columns on which we want to partition. We can ensure that data is evenly distributed across workers reducing skew and improving performance.\n",
    "\n",
    "`df = df. repartition (<n_partitions>, '‹col_1>', '‹col_2>',・・・）`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repartitioning by 'id' column...\n",
      "\n",
      "Number of rows per partition after repartitioning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/14 01:23:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0| 1110|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handle Data Skew - Repartition by Column\n",
    "# Repartition the DataFrame by the skewed column\n",
    "print(\"\\nRepartitioning by 'id' column...\")\n",
    "df_repartitioned = df.repartition(\"id\")\n",
    "# Check the new distribution\n",
    "print(\"\\nNumber of rows per partition after repartitioning:\")\n",
    "df_repartitioned.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition using salt\n",
    "```\n",
    "import pyspark.sql. functions as F\n",
    "# Add a 'salt' column with a random value for each row\n",
    "df = df.withColumn ('salt', F. rand ())\n",
    "# Repartition the DataFrame into 8 partitions based on the\n",
    "'salt' column\n",
    "df = df. repartition (8, 'salt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Salting involves adding a random value to your data to distribute it more evenly.`F.rand` function assigns a random float between zero and oneto each row in the data frame.This introduces randomness to distribute skewed keysmore evenly across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding a salt column for even distribution...\n",
      "\n",
      "Number of rows per partition after salting:\n",
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0|  123|\n",
      "|                   1|  127|\n",
      "|                   2|  129|\n",
      "|                   3|  146|\n",
      "|                   4|  133|\n",
      "|                   5|  133|\n",
      "|                   6|  167|\n",
      "|                   7|  152|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Handle Data Skew - Salting\n",
    "# Add a salt column to evenly distribute data\n",
    "print(\"\\nAdding a salt column for even distribution...\")\n",
    "df_salted = df.withColumn(\"salt\", F.rand())\n",
    "# Repartition by the salt column\n",
    "df_salted = df_salted.repartition(8, \"salt\")\n",
    "# Check the new distribution\n",
    "print(\"\\nNumber of rows per partition after salting:\")\n",
    "df_salted.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
