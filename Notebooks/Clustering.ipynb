{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0aee61",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Normalising datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15569b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "features_df = spark.createDataFrame([\n",
    "  (1, Vectors.dense ( [10.0, 10000.0, 1.0]),),\n",
    "  (2, Vectors.dense( [20.0, 30000.0, 2.01]),),\n",
    "  (3, Vectors.dense ( [30.0, 40000.0, 3.01]),)],\n",
    "  1, [\"id\",\"features\"])\n",
    "\n",
    "# print the first line of the df\n",
    "features_df.take(1)\n",
    "\n",
    "feature_scaler = MinMaxScaler (inputCol=\"features\", outputCol=\"sfeatures\")\n",
    "smodel = feature_scaler.fit (features_df)\n",
    "sfeatures_df = smodel.transform(features_df)\n",
    "\n",
    "#Look at the whole dataset\n",
    "sfeatures_df.select(\"features\" \"sfeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4542b",
   "metadata": {},
   "source": [
    "## Standardisation\n",
    "Mean =0, variance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f44b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "feature_stand_scaler = StandardScaler (inputCol=\"features\", outputCol=\"sfeatures\", withStd=True, withMean=True)\n",
    "stand_smodel = feature_stand_scaler.fit (features_df)\n",
    "stand_sfeatures_df = stand_smodel.transform (features_df)\n",
    "stand_sfeatures_df.take (1)\n",
    "stand_sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de978f",
   "metadata": {},
   "source": [
    "## Bucketize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d883f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [-float(\"inf\"), -10.0, 0.0, 10.0, float(\"inf\")]\n",
    "\n",
    "b_data = [ (-800.0, ), (-10.5, ), (-1.7,), (0.0,), (8.2, ), (90.1,) ]\n",
    "b_df = spark.createDataFrame(b_data, [\"features\"] )\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bfeatures\")\n",
    "bucketed_df = bucketizer.transform(b_df)\n",
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bbfab",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "### Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03995d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "sentences_df = spark.createDataFrame ([\n",
    "  (1, \"This is an introduction to Spark MLlib\"),\n",
    "  (2, \"MLlib includes libraries for classification and regression\"),\n",
    "  (3, \"It also contains supporting tools for pipelines\")],\n",
    "  [\"id\",\"sentence\"])\n",
    "\n",
    "sent_token = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "sent_tokenized_df = sent_token.transform(sentences_df)\n",
    "sent_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b2f70",
   "metadata": {},
   "source": [
    "### TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml. feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawfeatures\", numFeatures=20)\n",
    "# numFeatures is the number of features you want to keep track of\n",
    "\n",
    "sent_hfTF_df = hashingTF.transform(sent_tokenized_df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"idf_features\")\n",
    "idfModel = idf. fit(sent_hfTF_df)\n",
    "tfidf_df = idfModel.transform(sent_hfTF_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4280f",
   "metadata": {},
   "source": [
    "you get the TF and the IDF in 2 seperate columns. \n",
    "\n",
    "you probably need to do something to get the TF-IDF score for each word: https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6a94d",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fa95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"col1\", \"col2\", \"col3\"] , outputCol=\"features\" )\n",
    "vcluster_df = vectorAssembler.transform(cluster_df)\n",
    "vcluster_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa60b1",
   "metadata": {},
   "source": [
    "vector assembler creates a feature vector column which the kmeans algo will work with\n",
    "\n",
    "## Kmeans clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ff767",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(3)\n",
    "kmeans = kmeans.setSeed(1)\n",
    "kmodel = kmeans.fit(vcluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65618a78",
   "metadata": {},
   "source": [
    "choose the number of clusters, set the seed, then fit our data to the kmeans. You'll notice a couple of error messages, or warning messages here. This is just indicating that the BLAS library, which is a basic linear algebra library, wasn't able to load. That has no effect on the outcome. BLAS is useful for speeding up some linear algebra operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790712b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmodel. clusterCenters ( )\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675245df",
   "metadata": {},
   "source": [
    "## Heirarchal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d93ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkmeans = BisectingKMeans().setK(3)\n",
    "bkmeans = bkmeans.setSeed(1)\n",
    "\n",
    "bkmodel = bkmeans.fit(vcluster_df)\n",
    "bkcenters = bkmodel.clusterCenters()\n",
    "bkcenters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
